---
sidebar_position: 3
---

# 开发大语言模型需要什么？

:::info

本文主要内容来自论文 [A Survey of Large Language Models](https://arxiv.org/abs/2303.18223)。

:::

了解完大语言模型的原理之后，你可能会好奇 TA 是如何开发的。开发大语言模型的关键是什么。最近看到不少文章为了流量，甚至连 5G 通讯都说成了是开发大语言模型的关键 😂

其实从前面的原理介绍，不难看出，大语言模型的其中一个关键点是数据。

## 关键一：数据

训练数据主要是所谓的语料库。今天的很多语言模型的语料库主要有以下几种：

- Books：BookCorpus 是之前小语言模型如 GPT-2 常用的数据集，包括超过 11000 本电子书。主要包括小说和传记，最近更新时间是 2015 年 12 月。大一点的书籍语料库是 Gutenberg，它有 70000 本书，包括小说、散文、戏剧等作品，是目前最大的开源书籍语料库之一，最近更新时间是 2021 年 12 月。
- CommonCrawl：这个是目前最大的开源网络爬虫数据库，不过这个数据包含了大量脏数据，所以目前常用的四个数据库是 C4、CC-Stories、CC-News 和 RealNews。另外还有两个基于 CommonCrawl 提取的新闻语料库 REALNEWS 和 CC-News。
- Reddit Links：简单理解 Reddit 就是外国版本的百度贴吧 + 知乎。目前开源的数据库有 OpenWebText 和 [PushShift.io](http://PushShift.io)。
- Wikipedia：维基百科是目前大模型广泛使用的语料库。
- Code：一般包含 GitHub 上的开源公共代码库，或者是 StackOverflow 的数据，Google 之前发布过一个叫 BigQuery 的数据集。

实际上，训练大语言模型会混合使用不同的数据，一般不会只使用单一语料库。比如 GPT-3 用的混合数据集就包括 Books、CommonCrowl、Reddit Links 和 Wikipedia。

从数据上看，你需要知道一个事实，中文语料在这上面占比很小。ChatGPT 能用中文回答你的问题，只是因为它的语料里有一些中英文对应的语料，然后模型就像是「懂中文了一样」，所以更可能的情况是，它先生成了英文内容，再翻译成中文。如果你去问 ChatGPT 3.5：请给我几个中文谐音梗，你应该就能理解我说的话了。

以下是 ChatGPT 的返回结果：

```other
以下是一些中文谐音梗的例子：

我蒸发了，意思是我是 Vaper 了（Vaper发音接近蒸发）。

会有星期天，都得过过来，意思是星期二，星期三，星期四，星期五，星期六，都得过过来。

看到你，我的内心凉了 below zero，意思是凉了半截（below zero与半截音相近）。

蜀道难，难于上青天，赛道难，难于上488（488与“蜀道难”同音）。

洋葱姑娘，想哭却哭不出来，因为洋葱让我流泪了（洋葱和“要哭”在中文中的发音很相似）。

雨纷纷，旧故里草木深，我听闻，你始终在我身边深（“身边深”和“身边寻”谐音）。

你的车开得真快，一开就是“出租”，意思是“出祖宗”（发音类似）。
```

## 关键二：算法

有了数据之后，你还需要有算法去做计算，目前最常用的开发大语言模型的算法库有：

1. Transformers：这是一个使用 Transformer 架构构建的开源 Python 库。
2. DeepSpeed：是由微软开发的深度学习优化库。
3. Megatron-LM：这是由 Nvidia 开发的深度学习库。
4. JAX：它是由 Google 开发的用于高新能机器学习算法的 Python 库。

## 关键三：算力

简单理解，算力就是计算资源，或者说硬件，OpenAI 没有说它训练 GPT-3 语言模型花了多少计算资源。但 OpenAI 的 CEO 暗示硬件成本超过一亿美元，如果我们按照 1000 美元一个 GPU 计算，它大约使用了 10 万个 GPU，以 32 位运算为准，它能提供超过 100 PFLOPS 的算力，也就是每秒 10 亿亿次运算以上，这大约是阿里云最大的数据中心的四分之一的算力。

注意，这还是 GPT-3 时的花费。

另外，我还想分享一个观点，不要以为算力会随时间的前进，就能跨越。算力永远会是制约我们瓶颈，因为我们对人工智能的要求会不断的提高。
